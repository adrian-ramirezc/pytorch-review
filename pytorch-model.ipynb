{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST( \"\", \n",
    "                        train = True, \n",
    "                        download=True, \n",
    "                        transform=transforms.Compose(\n",
    "                            [transforms.ToTensor()]\n",
    "                                                    )\n",
    "                      )\n",
    "\n",
    "test = datasets.MNIST( \"\", \n",
    "                        train = False, \n",
    "                        download=True, \n",
    "                        transform=transforms.Compose(\n",
    "                            [transforms.ToTensor()]\n",
    "                                                    )\n",
    "                      )\n",
    "\n",
    "# batches : How many samples at a time will be processed\n",
    "# shuffle : Miw the data in each batch \n",
    "#           (in order to avoid only one label type in the batch)\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = 10)\n",
    "testset  = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN as a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Object Oriented Library\n",
    "import torch.nn as nn \n",
    "\n",
    "# Functions Library \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Parent class init\n",
    "        super().__init__()\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim = 1) \n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0312, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5430, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0232, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 1e-3)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of features and labels\n",
    "        X, y = data\n",
    "        # before passing data through the net\n",
    "        net.zero_grad() # set grad to zero\n",
    "\n",
    "        # feedforward\n",
    "        output = net(X.view(-1,28*28))\n",
    "\n",
    "        # loss\n",
    "        loss = F.nll_loss(output, y)\n",
    "\n",
    "        # backforward\n",
    "        loss.backward()\n",
    "\n",
    "        # update weigths\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.965\n"
     ]
    }
   ],
   "source": [
    "correct = 0 \n",
    "total = 0 \n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sample = X[9]\n",
    "\n",
    "plt.imshow(img_sample.view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(img_sample.view(-1,784))[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88f611f6ea6cd74939d7b3949f72fcb0438a4e53f46e548f8b293cd3c259247d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
